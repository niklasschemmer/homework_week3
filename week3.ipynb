{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNI6g/wKbcf8GAKjPnd/HAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niklasschemmer/homework_week3/blob/main/week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19-zXM80tRa-"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-bJNFIgub2P"
      },
      "source": [
        "train_dataset, test_dataset = tfds.load('genomics_ood', split=['train', 'test'], as_supervised=True)\n",
        "\n",
        "def convert_genomic_string_to_number(sequence):\n",
        "  vocab = {'A':'1', 'C':'2', 'G':'3', 'T':'0'}\n",
        "  for key in vocab.keys():\n",
        "    sequence = tf.strings.regex_replace(sequence, key, vocab[key])\n",
        "  split = tf.strings.bytes_split(sequence)\n",
        "  labels = tf.cast(tf.strings.to_number(split), tf.uint8)\n",
        "  onehot = tf.one_hot(labels, 4)\n",
        "  onehot = tf.reshape(onehot, (-1,))\n",
        "  return onehot\n",
        "\n",
        "def prepare_genomics_data_training(genomics_ood):\n",
        "  genomics_ood = genomics_ood.take(100000)\n",
        "  #flatten the images into vectors\n",
        "  genomics_ood = genomics_ood.map(lambda sequence, target: (convert_genomic_string_to_number(sequence), target))\n",
        "  genomics_ood = genomics_ood.map(lambda sequence, target: (sequence, tf.one_hot(target, depth=10)))\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  genomics_ood = genomics_ood.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  genomics_ood = genomics_ood.shuffle(100)\n",
        "  genomics_ood = genomics_ood.batch(128)\n",
        "  genomics_ood = genomics_ood.prefetch(50)\n",
        "  #return preprocessed dataset\n",
        "  return genomics_ood\n",
        "\n",
        "def prepare_genomics_data_test(genomics_ood):\n",
        "  genomics_ood = genomics_ood.take(1000)\n",
        "  #flatten the images into vectors\n",
        "  genomics_ood = genomics_ood.map(lambda sequence, target: (convert_genomic_string_to_number(sequence), target))\n",
        "  genomics_ood = genomics_ood.map(lambda sequence, target: (sequence, tf.one_hot(target, depth=10)))\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  genomics_ood = genomics_ood.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  genomics_ood = genomics_ood.shuffle(100)\n",
        "  genomics_ood = genomics_ood.batch(128)\n",
        "  genomics_ood = genomics_ood.prefetch(50)\n",
        "  #return preprocessed dataset\n",
        "  return genomics_ood\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk40U6t6uUNS"
      },
      "source": [
        "train_dataset = train_dataset.apply(prepare_genomics_data_training)\n",
        "test_dataset = test_dataset.apply(prepare_genomics_data_test)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnNmS3hDCb4Y"
      },
      "source": [
        "class CustomDense(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, activation_funtion, units):\n",
        "        super(CustomDense, self).__init__()\n",
        "        self.units = units\n",
        "        self.activation = activation_funtion\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "\n",
        "    def call(self, inputs): \n",
        "        x = tf.matmul(inputs, self.w) + self.b\n",
        "        x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdEVx6kgBt0l"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = CustomDense(tf.nn.sigmoid, 256)\n",
        "        self.dense2 = CustomDense(tf.nn.sigmoid, 256)\n",
        "        self.out = CustomDense(tf.nn.softmax, 10)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIlACWGkEJ8M"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTIePkAvEsNa"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize the model.\n",
        "model = MyModel()\n",
        "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "    \n",
        "    #track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "    #testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCaia0cJWiyG"
      },
      "source": [
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "line3, = plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend((line1,line2, line3),(\"training loss\",\"test loss\", \"test accuracy\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}